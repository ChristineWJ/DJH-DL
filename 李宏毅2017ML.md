# 李宏毅机器学习(2017)

[TOC]



## Introduction of ML

- ML : Looking for a function from data

- Application

  - Speech Recognition
  - Image Recognition
  - Play Go
  - Dialogue System

- FrameWork

  - step1:a set of function 假设空间（space of model）
  - step2:traning data -> goodness of fucntion f,pick the best function as f
  - step3:testing

- Learning map

  - supervisied learning 

    - regression = predict a series of output
    - classification =  output label
      - binary : spam filter,tumour classification
      - multi-calss : text classification
    - model : linear model,no-linear model(deep learning and svm decision tree ,knn et.)

  - semi-supervisied learning

  - unsupervisied learning

  - transfer learning

  - reinforcement learning

    - difference with supervisied learning

      > supervisied learning : learning from teacher
      >
      > reinforcement learning : learning from critics
      >
      > Alpha go = supervisied learning + reinforcement learning

## Regression

- Output a scalar

  - eg : stock market forecast , slef-drive car , recommandation system

- Application Example : estimating the combat power (cp) of a pokemon after evolution预测宝可梦进化后的战力

  - step1 :model 

    - a set of fuction(f1,f2,f3,f4....)
    - y=wx+b linear model

  - step2 :goodness of function 

    - y^=wx+b

    - L(f) = L(w,b) = loss function : compute the loss of output and y^

    - $$
      L(f)=\sum {_1^n}(y-(wx+b))^2
      $$

  - step3:pick the best function

    - compute the minimum of  loss function

  - $$
    f^*=argminL(f)
    $$

    $$
    w^*,b^*=argminL(f)
    $$

    $$
    w^*,b^*=argminL(f)=argmin(\sum {_1^n}(y-(wx+b))^2)
    $$

    - gradient descent 

      > consider loss function L(w) with one parameter w [w,b]:
      >
      > pick an inital value w^0 [delay on local if there is a  minimum or global minumum ]
      >
      > compute 
      > $$
      > \dfrac{dL}{dw}|w=w^{0} 
      > $$
      > update makes L(f) smaller [condider the function type(convex function ,nun-convex function)]
      > $$
      > W^{n+1}=w^{n}-\alpha\dfrac {dL}{1w}
      > $$
      >  the learning rate
      > $$
      > \alpha
      > $$
      >

  - step4:testing (generalization)

  - step5:model selection

    > a more complex model can lower error on trainning data,but
    >
    > a more complex model does not always lead to better performance on testing data ( **overfitting**)

- Overfitting

  - concept

    >  model has a performance good on trianing data **but **bad performance on testing data

  - resolution

    > redesign the model
    >
    > regularization L2 
    > $$
    > L(f)=\sum {_1^n}(y-(wx+b))^2+regularization
    > $$
    >
    > - $$
    >   regularization=\lambda\sum(wx)^2
    >   $$
    >
    > - we believe smoother function is more likely to bt correct

- asd 

- asd 



































